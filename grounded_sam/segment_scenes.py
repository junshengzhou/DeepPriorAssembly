# -*- coding: utf-8 -*-
"""grounded_sam_colab_demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/betogaona7/Grounded-Segment-Anything/blob/main/grounded_sam_colab_demo.ipynb

![Grounded SAM Inpainting Demo](https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/main/assets/grounded_sam_inpainting_demo.png)

## Why this project?

- [Segment Anything](https://github.com/facebookresearch/segment-anything) is a strong segmentation model. But it need prompts (like boxes/points) to generate masks.
- [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) is a strong zero-shot detector which enable to generate high quality boxes and labels with free-form text.
- The combination of the two models enable **to detect and segment everything** with text inputs!

## Install
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

# !git clone https://github.com/IDEA-Research/Grounded-Segment-Anything

# # %cd /content/Grounded-Segment-Anything
# !pip install -q -r requirements.txt
# # %cd /content/Grounded-Segment-Anything/GroundingDINO
# !pip install -q .
# # %cd /content/Grounded-Segment-Anything/segment_anything
# !pip install -q .
# # %cd /content/Grounded-Segment-Anything

"""## Imports"""

import os, sys

sys.path.append(os.path.join(os.getcwd(), "GroundingDINO"))

import argparse
import copy

from IPython.display import display
from PIL import Image, ImageDraw, ImageFont
from torchvision.ops import box_convert

# Grounding DINO
import GroundingDINO.groundingdino.datasets.transforms as T
from GroundingDINO.groundingdino.models import build_model
from GroundingDINO.groundingdino.util import box_ops
from GroundingDINO.groundingdino.util.slconfig import SLConfig
from GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap
from GroundingDINO.groundingdino.util.inference import annotate, load_image, predict
from skimage.transform import resize

import supervision as sv

# segment anything
from segment_anything import build_sam, SamPredictor
import cv2
import numpy as np
import matplotlib.pyplot as plt

# diffusers
import PIL
import requests
import torch
from io import BytesIO


from huggingface_hub import hf_hub_download

def download_image(url, image_file_path):
    r = requests.get(url, timeout=4.0)
    if r.status_code != requests.codes.ok:
        assert False, 'Status code error: {}.'.format(r.status_code)

    with Image.open(BytesIO(r.content)) as im:
        im.save(image_file_path)
    print('Image downloaded from url: {} and saved to: {}.'.format(url, image_file_path))


def load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu'):

    args = SLConfig.fromfile('ckpts/GroundingDINO_SwinB.cfg.py')

    args.device = device
    model = build_model(args)

    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)


    checkpoint = torch.load(cache_file, map_location=device)
    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)
    print("Model loaded from {} \n => {}".format(cache_file, log))
    _ = model.eval()
    return model


def segment(image, sam_model, boxes):
  sam_model.set_image(image)
  H, W, _ = image.shape
  boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])

  transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(device), image.shape[:2])
  masks, _, _ = sam_model.predict_torch(
      point_coords = None,
      point_labels = None,
      boxes = transformed_boxes,
      multimask_output = False,
      )
  return masks.cpu()


def draw_mask(mask, image, random_color=True):
    if random_color:
        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)
    else:
        color = np.array([30/255, 144/255, 255/255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)

    annotated_frame_pil = Image.fromarray(image).convert("RGBA")
    mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert("RGBA")

    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))

def crop_enlarge_and_center_mask(image, mask, scale=0.9, target_size=(1137, 1137)):

    mask = mask[:,:,0]

    nonzero_coords = np.argwhere(mask == 1)

    if len(nonzero_coords) == 0:
        return mask
    
    min_coords = np.min(nonzero_coords, axis=0)
    max_coords = np.max(nonzero_coords, axis=0)
    min_row, min_col = min_coords
    max_row, max_col = max_coords
    
    height = max_row - min_row + 1
    width = max_col - min_col + 1
    
    target_height, target_width = target_size
    
    new_mask = np.zeros((target_height, target_width, 1), dtype=mask.dtype)
    new_image = np.ones((target_height, target_width, 3), dtype=mask.dtype) * 255
    
    scale_factor = min(target_height / height, target_width / width) * scale
    
    new_height = int(height * scale_factor)
    new_width = int(width * scale_factor)

    scaled_nonzero_region_mask = resize(mask[min_row:max_row+1, min_col:max_col+1], (new_height, new_width, 1))

    scaled_nonzero_region_image = resize(image[min_row:max_row+1, min_col:max_col+1, :], (new_height, new_width, 3)) * 255.0

    new_row_start = (target_height - new_height) // 2
    new_col_start = (target_width - new_width) // 2
    
    new_image[new_row_start:new_row_start+new_height, new_col_start:new_col_start+new_width, :] = scaled_nonzero_region_image.copy()
    new_mask[new_row_start:new_row_start+new_height, new_col_start:new_col_start+new_width, :] = scaled_nonzero_region_mask.copy()
    
    mask_inverted = 1-new_mask
    masked_region_white = mask_inverted * 255.0

    image_seg = new_mask * new_image + masked_region_white

    return image_seg


# detect object using grounding DINO
def detect(image, text_prompt, model, box_threshold = 0.3, text_threshold = 0.25):
  boxes, logits, phrases = predict(
      model=model,
      image=image,
      caption=text_prompt,
      box_threshold=box_threshold,
      text_threshold=text_threshold
  )

  annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)
  annotated_frame = annotated_frame[...,::-1] # BGR to RGB
  return annotated_frame, boxes, logits, phrases

parser = argparse.ArgumentParser()

parser.add_argument(
    "--input_path",
    type=str,
    default="",
    help="the prompt to render"
)

parser.add_argument(
    "--save_path",
    type=str,
    default=""
)

parser.add_argument(
    "--image_id",
    type=str,
    default=""
)


opt = parser.parse_args()


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

"""### Grounding DINO model"""

ckpt_repo_id = "ShilongLiu/GroundingDINO"
ckpt_filenmae = "groundingdino_swinb_cogcoor.pth"
ckpt_config_filename = "GroundingDINO_SwinB.cfg.py"

# ----------------------------------------Load models------------------------------------------

groundingdino_model = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename, device)

"""### SAM"""

# ! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

sam_checkpoint = 'ckpts/sam_vit_h_4b8939.pth'

sam_predictor = SamPredictor(build_sam(checkpoint=sam_checkpoint).to(device))

"""## Inference"""


cate_labels = 'Sofa.Chair.Table.Bed.Cabinet.Dresser.Ashcan.Television.Piano.Lamp'


# ----------------------------------------Load image------------------------------------------
data_tmp = opt.image_id
opt.input_path = opt.input_path + '/%s.png' % (data_tmp)

local_image_path = opt.input_path

opt.save_path = os.path.join(opt.save_path, data_tmp)

os.makedirs(opt.save_path, exist_ok=True)


image_source, image = load_image(local_image_path)
image_source_tmp = Image.fromarray(image_source)
image_source_tmp.save("%s/ori_image.jpg" % opt.save_path)

"""## Grounding DINO for detection"""

# ----------------------------------------Detect and segment------------------------------------------

annotated_frame, detected_boxes, logits, phrases = detect(image, text_prompt=cate_labels, model=groundingdino_model)
image_detect = Image.fromarray(annotated_frame)
image_detect.save("%s/test_det.jpg" % opt.save_path, format="JPEG", quality=80)

"""## SAM for segmentation"""

segmented_frame_masks = segment(image_source, sam_predictor, boxes=detected_boxes)


annotated_frame_with_mask = annotated_frame
for i in range(len(segmented_frame_masks)):
    annotated_frame_with_mask = draw_mask(segmented_frame_masks[i][0], annotated_frame_with_mask)

image_seg = Image.fromarray(annotated_frame_with_mask)

image_seg.save("%s/test_seg.png" % opt.save_path)

mask_all = []

for i in range(len(segmented_frame_masks)):

    s = 6

    if logits[i] < 0.4:
        continue

    mask = segmented_frame_masks[i][0].detach().cpu().numpy()

    mask_all.append(mask[np.newaxis,:,:])

    mask = mask[:,:,np.newaxis]
    mask_inverted = 1 - mask
    masked_region_white = 255 * mask_inverted

    image_seg = mask * image_source + masked_region_white
    image_seg = crop_enlarge_and_center_mask(image_source, scale=s/10.0, mask=mask, target_size=image_source.shape[:2])
    segmented_image_pil = Image.fromarray(image_seg.astype(np.uint8))
    save_path = f'{opt.save_path}/seg_{i}_{phrases[i]}.png'
    segmented_image_pil.save(save_path)

np.save("%s/mask_all.npy" % opt.save_path, mask_all)

